{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6337c0b",
   "metadata": {},
   "source": [
    "# Custom Knowledge Chatbot w/ LlamaIndex\n",
    "By Liam Ottley - YouTube: https://www.youtube.com/@LiamOttley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8911e71",
   "metadata": {},
   "source": [
    "Examples:\n",
    "- https://gita.kishans.in/\n",
    "- https://www.chatpdf.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c425f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in c:\\users\\sage\\anaconda3\\lib\\site-packages (0.4.33)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from llama_index) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\sage\\anaconda3\\lib\\site-packages (from llama_index) (0.5.7)\n",
      "Requirement already satisfied: langchain in c:\\users\\sage\\anaconda3\\lib\\site-packages (from llama_index) (0.0.152)\n",
      "Requirement already satisfied: openai>=0.26.4 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from llama_index) (0.27.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\sage\\anaconda3\\lib\\site-packages (from llama_index) (1.5.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\sage\\anaconda3\\lib\\site-packages (from llama_index) (0.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sage\\anaconda3\\lib\\site-packages (from llama_index) (1.21.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sage\\anaconda3\\lib\\site-packages (from openai>=0.26.4->llama_index) (4.62.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sage\\anaconda3\\lib\\site-packages (from openai>=0.26.4->llama_index) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from openai>=0.26.4->llama_index) (2.28.1)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from dataclasses-json->llama_index) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from dataclasses-json->llama_index) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from dataclasses-json->llama_index) (0.8.0)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain->llama_index) (2.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain->llama_index) (4.0.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain->llama_index) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain->llama_index) (1.10.7)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain->llama_index) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>1.3 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain->llama_index) (1.4.39)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from pandas->llama_index) (2021.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from tiktoken->llama_index) (2022.7.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp->openai>=0.26.4->llama_index) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp->openai>=0.26.4->llama_index) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp->openai>=0.26.4->llama_index) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp->openai>=0.26.4->llama_index) (2.0.4)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain->llama_index) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from requests>=2.20->openai>=0.26.4->llama_index) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>1.3->langchain->llama_index) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sage\\anaconda3\\lib\\site-packages (from tqdm->openai>=0.26.4->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json->llama_index) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\sage\\anaconda3\\lib\\site-packages (0.0.152)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>1.3 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (4.62.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from langchain) (1.21.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>1.3->langchain) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sage\\anaconda3\\lib\\site-packages (from tqdm>=4.48.0->langchain) (0.4.6)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (22.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sage\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sage\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041eae8",
   "metadata": {},
   "source": [
    "# Basic LlamaIndex Usage Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6395b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf41880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98df5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 1321 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create an index of your documents\n",
    "\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "\n",
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18731b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1448 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think Facebook's LLaMa is a great step forward in democratizing access to large language models and advancing research in this subfield of AI. It is encouraging to see that they are making the model available at several sizes and providing a model card to detail how it was built in accordance with responsible AI practices. I am also glad to see that they are releasing the model under a noncommercial license to ensure integrity and prevent misuse.\n"
     ]
    }
   ],
   "source": [
    "# Query your index!\n",
    "\n",
    "response = index.query(\"What do you think of Facebook's LLaMa?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57864389",
   "metadata": {},
   "source": [
    "# Customize your LLM for different output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c726dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 1321 tokens\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from llama_index import LLMPredictor, GPTSimpleVectorIndex, PromptHelper\n",
    "import openai\n",
    "\n",
    "\n",
    "# define LLM\n",
    "class OpenAILanguageModel(openai.Completion):\n",
    "    def generate_prompt(self, prompt: str, max_tokens: int, **kwargs) -> str:\n",
    "        response = self.create_prompt(prompt=prompt, max_tokens=max_tokens, **kwargs)\n",
    "        return response.choices[0].text\n",
    "\n",
    "    def agenerate_prompt(self, prompt: str, max_tokens: int, **kwargs) -> str:\n",
    "        response = self.create_prompt(prompt=prompt, max_tokens=max_tokens, **kwargs)\n",
    "        return [choice.text for choice in response.choices]\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=OpenAILanguageModel(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "custom_LLM_index = GPTSimpleVectorIndex(\n",
    "    documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f359b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1448 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think Facebook's LLaMa is a great step forward in democratizing access to large language models and advancing research in this subfield of AI. It is encouraging to see that they are making the model available at several sizes and providing a model card to detail how it was built in accordance with responsible AI practices. I am also glad to see that they are releasing the model under a noncommercial license to ensure integrity and prevent misuse.\n"
     ]
    }
   ],
   "source": [
    "# Query your index!\n",
    "\n",
    "response = custom_LLM_index.query(\"What do you think of Facebook's LLaMa?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4e5c",
   "metadata": {},
   "source": [
    "# Wikipedia Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db9772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "wikidocs = loader.load_data(pages=['Cyclone Freddy'])\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Cyclone_Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941c9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 7896 tokens\n"
     ]
    }
   ],
   "source": [
    "wiki_index = GPTSimpleVectorIndex(wikidocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a4dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3843 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cyclone Freddy is a very intense tropical cyclone that traversed the southern Indian Ocean for more than five weeks in February and March 2023. It is both the longest-lasting and highest-ACE-producing tropical cyclone ever recorded worldwide. Additionally, it is the third-deadliest tropical cyclone recorded in the Southern Hemisphere, only behind 2019's Cyclone Idai and the 1973 Flores cyclone. It caused catastrophic flooding, wind damage, and loss of life in Madagascar, Mauritius, Réunion, Mozambique, and Malawi. In Mauritius, the cyclone passed within 200 km (120 mi) of the island, just north of Grand Bay. Strong winds and waves were observed along the northern coast of Mauritius. Winds in Port Louis reached 104 km/h (65 mph) while a peak gust of 154 km/h (96 mph) was observed on Signal Mountain. Flooding and gale-force winds also affected the country, resulting in one fatality and at least 500 displaced families in a variety of shelters across Mauritius. Contact was also lost with the Taiwanese-flagged fishing trawler LV Lien Sheng Fa with a crew of 16 just outside the territorial waters of Mauritius.\n"
     ]
    }
   ],
   "source": [
    "response = wiki_index.query(\"What is cyclone freddy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2acc",
   "metadata": {},
   "source": [
    "# Customer Support Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f396a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./asos').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(\"What premier service options do I have in the UAE?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77e646",
   "metadata": {},
   "source": [
    "# YouTube Video Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89160742",
   "metadata": {},
   "outputs": [],
   "source": [
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=K7Kh9Ntd8VE&ab_channel=DaveNick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e88fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = index.query(\"What some YouTube automation mistakes to avoid?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a05da",
   "metadata": {},
   "source": [
    "# Chatbot Class - Just include your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1baf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(\"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\", index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cda5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
